{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-06T02:19:35.728196Z","iopub.status.idle":"2022-01-06T02:19:35.728512Z","shell.execute_reply.started":"2022-01-06T02:19:35.728326Z","shell.execute_reply":"2022-01-06T02:19:35.728341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport scipy.stats","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:19:58.127710Z","iopub.execute_input":"2022-01-06T02:19:58.128552Z","iopub.status.idle":"2022-01-06T02:19:59.028577Z","shell.execute_reply.started":"2022-01-06T02:19:58.128507Z","shell.execute_reply":"2022-01-06T02:19:59.027800Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"flwrs = pd.read_csv(\"../input/ig-data/followers_views.csv\")\nusrs = pd.read_csv(\"../input/ig-data/usersData.csv\")\nlikes = pd.read_csv(\"../input/ig-data/likes_views.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:19:59.498092Z","iopub.execute_input":"2022-01-06T02:19:59.498915Z","iopub.status.idle":"2022-01-06T02:20:00.560497Z","shell.execute_reply.started":"2022-01-06T02:19:59.498878Z","shell.execute_reply":"2022-01-06T02:20:00.559657Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"flwrs.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:00.811825Z","iopub.execute_input":"2022-01-06T02:20:00.812098Z","iopub.status.idle":"2022-01-06T02:20:00.831149Z","shell.execute_reply.started":"2022-01-06T02:20:00.812070Z","shell.execute_reply":"2022-01-06T02:20:00.830389Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"usrs.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:01.776607Z","iopub.execute_input":"2022-01-06T02:20:01.776952Z","iopub.status.idle":"2022-01-06T02:20:01.796651Z","shell.execute_reply.started":"2022-01-06T02:20:01.776914Z","shell.execute_reply":"2022-01-06T02:20:01.795967Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"likes.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:02.457546Z","iopub.execute_input":"2022-01-06T02:20:02.458320Z","iopub.status.idle":"2022-01-06T02:20:02.466784Z","shell.execute_reply.started":"2022-01-06T02:20:02.458273Z","shell.execute_reply":"2022-01-06T02:20:02.466067Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"flwrs.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:03.123612Z","iopub.execute_input":"2022-01-06T02:20:03.124484Z","iopub.status.idle":"2022-01-06T02:20:03.135463Z","shell.execute_reply.started":"2022-01-06T02:20:03.124447Z","shell.execute_reply":"2022-01-06T02:20:03.134874Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"usrs.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:03.705294Z","iopub.execute_input":"2022-01-06T02:20:03.705603Z","iopub.status.idle":"2022-01-06T02:20:03.715826Z","shell.execute_reply.started":"2022-01-06T02:20:03.705574Z","shell.execute_reply":"2022-01-06T02:20:03.714998Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"likes.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:04.510702Z","iopub.execute_input":"2022-01-06T02:20:04.511530Z","iopub.status.idle":"2022-01-06T02:20:04.522920Z","shell.execute_reply.started":"2022-01-06T02:20:04.511492Z","shell.execute_reply":"2022-01-06T02:20:04.521944Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"So we have the three dataframes loaded - no null values found. Let's now graph the data from the paper. This is:\n1. log average views per number of instagrammers\n2. followers per views\n3. likes per views\n","metadata":{}},{"cell_type":"code","source":"df = np.log(usrs.avg_views) #log transform column first\ndf.hist(bins=100) #make histogram\n\nplt.xscale(\"log\")\nplt.xlabel(\"Log average views\")\nplt.ylabel(\"Number of instagrammers\")\n\n#add mean line\nplt.axvline(x=df.mean(),linestyle='--',color='red')\n\nprint(usrs.avg_views.mean()) #average views\n#print(np.exp(np.log(usrs.avg_views.mean()))) #why does this not work\nprint(np.exp(df.mean())) #average log views\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:06.502215Z","iopub.execute_input":"2022-01-06T02:20:06.502667Z","iopub.status.idle":"2022-01-06T02:20:07.299935Z","shell.execute_reply.started":"2022-01-06T02:20:06.502626Z","shell.execute_reply":"2022-01-06T02:20:07.298377Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"From the paper, we can fit a normal distribution to this data:\n\"Furthermore,as this distribution is so close to normal, we ascertain that our selection of sampled Instagrammers is a good semblance of real-world\ninfluence with micro-influencers populating the dense mean and casual users and celebrities appearing at the distribution extremes\"","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom scipy.stats import norm\n\n# Fit Gaussian distribution and plot\nsns.distplot(df, fit=norm, kde=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:10.706675Z","iopub.execute_input":"2022-01-06T02:20:10.706940Z","iopub.status.idle":"2022-01-06T02:20:11.188339Z","shell.execute_reply.started":"2022-01-06T02:20:10.706903Z","shell.execute_reply":"2022-01-06T02:20:11.187493Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"For the statistics, we need to adjust the data used according to the paper:\n\"To avoid these sorts of odd behaviors, we performed univariate\noutliers removal, ignoring the top and bottom posts for users with\nposts statistics above 2 standard deviations.\"","metadata":{}},{"cell_type":"code","source":"from scipy.stats import zscore\n\nz_scores = scipy.stats.zscore(flwrs.views)\n#calculate z-scores of dataframe\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 2)\nnew_df = flwrs[filtered_entries]\n\nprint(flwrs.views.mean())\nprint(new_df.views.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:16.031833Z","iopub.execute_input":"2022-01-06T02:20:16.032119Z","iopub.status.idle":"2022-01-06T02:20:16.104510Z","shell.execute_reply.started":"2022-01-06T02:20:16.032083Z","shell.execute_reply":"2022-01-06T02:20:16.103909Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"plt.scatter(new_df.followers, new_df.views)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Followers\")\nplt.ylabel(\"Views\")\n\nplt.xlim(0.1, 10**9)  \nplt.ylim(0.1, 10**8) \n\n#add mean line\n#plt.axvline(x=new_df.mean(),linestyle='--',color='red')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:20:18.137924Z","iopub.execute_input":"2022-01-06T02:20:18.138621Z","iopub.status.idle":"2022-01-06T02:20:20.968388Z","shell.execute_reply.started":"2022-01-06T02:20:18.138587Z","shell.execute_reply":"2022-01-06T02:20:20.967717Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"So it seems like they used all the data for these graphs, and did not remove the >2 st.dev outliers. Let's therefore graph those:","metadata":{}},{"cell_type":"code","source":"plt.scatter(flwrs.followers, flwrs.views)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Followers\")\nplt.ylabel(\"Views\")\n\nplt.xlim(0.1, 10**9)  \nplt.ylim(0.1, 10**8) \nplt.show()\n\n#add mean line\n#plt.axvline(x=flwrs.followers,y = flwrs.views, linestyle='--',color='red')\n\nm, b = np.polyfit(flwrs.followers,flwrs.views, 1)\n\nx=flwrs.followers\ny=flwrs.views\nplt.plot(x, y, '.')\nplt.plot(x, m*x + b)\nplt.ylim(0.1, 0.5*10**8) \nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Followers\")\nplt.ylabel(\"Views\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:25:40.794790Z","iopub.execute_input":"2022-01-06T02:25:40.795121Z","iopub.status.idle":"2022-01-06T02:25:47.138946Z","shell.execute_reply.started":"2022-01-06T02:25:40.795083Z","shell.execute_reply":"2022-01-06T02:25:47.138024Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"z_scores = scipy.stats.zscore(likes.views)\n#calculate z-scores of dataframe\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 2)\nnew_df_likes_views = likes[filtered_entries]\n\nprint(likes.views.mean())\nprint(new_df_likes_views.views.mean())","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:27:19.036920Z","iopub.execute_input":"2022-01-06T02:27:19.037195Z","iopub.status.idle":"2022-01-06T02:27:19.090236Z","shell.execute_reply.started":"2022-01-06T02:27:19.037167Z","shell.execute_reply":"2022-01-06T02:27:19.089420Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"plt.plot(new_df_likes_views.likes,new_df_likes_views.views,\".\")\n\nplt.xlabel(\"Engagement\")\nplt.ylabel(\"Views\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlim(0.1, 10**7)  \nplt.ylim(0.1, 10**7) \nplt.show()\n\n\nplt.plot(likes.likes,likes.views,\".\")\nplt.xlabel(\"Engagement\")\nplt.ylabel(\"Views\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlim(0.1, 10**7)  \nplt.ylim(0.1, 10**8) \nplt.show()\n\nm, b = np.polyfit(likes.likes,likes.views, 1)\nx,y = likes.likes,likes.views\nplt.plot(x, y, '.')\nplt.plot(x, m*x + b)\nplt.ylim(0.1, 10**8) \nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Likes\")\nplt.ylabel(\"Views\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:38:41.540810Z","iopub.execute_input":"2022-01-06T02:38:41.541084Z","iopub.status.idle":"2022-01-06T02:38:52.576251Z","shell.execute_reply.started":"2022-01-06T02:38:41.541055Z","shell.execute_reply":"2022-01-06T02:38:52.575692Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#create scatterplot with regression line and confidence interval lines\ng=sns.regplot(x=x, y=y, fit_reg=True, ci=None)\ng.set(ylim(0, 5*10**7))\ng.set(xlim(0,10**7))\n\ng.set(xscale(\"log\"))\ng.set(yscale(\"log\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:51:30.323009Z","iopub.execute_input":"2022-01-06T02:51:30.323856Z","iopub.status.idle":"2022-01-06T02:51:35.991559Z","shell.execute_reply.started":"2022-01-06T02:51:30.323796Z","shell.execute_reply":"2022-01-06T02:51:35.990394Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\n\n#select data\nX, y = usrs.drop(\"avg_views\", 1), usrs.avg_views\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n#fit to RFR\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=0).fit(X_train, y_train)\n\n#R2 scores for train and test data\nprint(model.score(X_train, y_train))\nprint(model.score(X_test, y_test))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-06T03:02:23.118281Z","iopub.execute_input":"2022-01-06T03:02:23.118988Z","iopub.status.idle":"2022-01-06T03:03:49.739726Z","shell.execute_reply.started":"2022-01-06T03:02:23.118941Z","shell.execute_reply":"2022-01-06T03:03:49.738694Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(random_state=0).fit(X_train, y_train)\n\n#R2 scores for train and test data\nprint(model.score(X_train, y_train))\nprint(model.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T03:04:52.285223Z","iopub.execute_input":"2022-01-06T03:04:52.285545Z","iopub.status.idle":"2022-01-06T03:04:52.357717Z","shell.execute_reply.started":"2022-01-06T03:04:52.285510Z","shell.execute_reply":"2022-01-06T03:04:52.356782Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}